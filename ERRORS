# AMLT

# importing libraries
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Accessing dataset
customer=pd.read_csv('Mall_customers.csv')

customer

### PREPROCESSING:

# Dropping the columns ‘CustomerID’ .
customer=customer.drop(['CustomerID'],axis=1)
customer

# Checking if there are any null values
customer.isnull().sum()

# removing all records having null values
customer.dropna(inplace=True)

customer.isnull().sum()

 # Drop all rows having Family_Size values as 7.0, 8.0 and 9.0

index = customer[customer['Age'].isin([7.0, 8.0, 9.0])].index

# Convereting categorical to numeric
cust=pd.get_dummies(customer)
cust

y=cust['Spending Score (1-100)']

X=cust.drop(['Spending Score (1-100)'],axis=1)

# Standardisation 
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_scaled=sc.fit_transform(X)
X_scaled

# COnverting to DF
X=pd.DataFrame(X, columns=X.columns)
X

y

## DIMENSIONALITY REDUCTION

## PCA:

The basic idea of PCA is to find a lower-dimensional representation of a high-dimensional dataset while retaining as much of the original information as possible.

PCA works by identifying the principal components of a dataset, which are the directions in the data that explain the most variance. These principal components are calculated as linear combinations of the original features in the dataset, and they are ordered so that the first principal component explains the most variance, the second explains the second most variance, and so on.

PCA can also be used for data preprocessing, where it can help to reduce the noise and redundancy in the data, making it easier to analyze using other techniques.

### USING EIGEN VALUES

# Construction of covariance marix

cm=np.cov(X_scaled.T)
cm

cm.shape

# Finding eigen value, eigen vector

eig_val,eig_vec=np.linalg.eig(cm)
eig_val

eig_vec

#Sorting eigen values
sorted_eig_val=[i for i in sorted(eig_val, reverse=True)]
sorted_eig_val

# Choosing the dimension =2

tot=sum(sorted_eig_val)
tot

exp_var=[(i/tot) for i in sorted_eig_val]
exp_var
# Explained variance

len(exp_var)

cum_exp_var=np.cumsum(exp_var)
cum_exp_var

plt.bar(range(1,5), exp_var,label='Explained Variance')
plt.xlabel('Principal Component')
plt.ylabel(' Explained Variance')
plt.legend();

# range(1,5) is decided by number of values in exp_var, range(1,5) means 4 values 

# Construction of projection matrix
eigen_pair=[(np.abs(eig_val[i]),eig_vec[:,i]) for i in range(len(eig_val))]
eigen_pair

# Taking only 2 dimension

w=np.hstack((eigen_pair[0][1][:,np.newaxis],
            eigen_pair[1][1][:,np.newaxis]))

#### To take 6 dimensions:

w = np.hstack((eigen_pair[0][1][:, np.newaxis],
               eigen_pair[1][1][:, np.newaxis],
               eigen_pair[2][1][:, np.newaxis],
               eigen_pair[3][1][:, np.newaxis],
               eigen_pair[4][1][:, np.newaxis],
               eigen_pair[5][1][:, np.newaxis]))


w

w.shape

# here 4 is previous dimension

# Transforming 95 dim data to 2 dim

X_scaled.shape

w.shape

new_X=X_scaled.dot(w)
new_X

new_X.shape

# Visualising the projected data

for l in np.unique(y):
    plt.scatter(new_X[y==1,0], new_X[y==1,1],marker='s')
    plt.scatter(new_X[y==2,0], new_X[y==2,1],marker='x')
    plt.scatter(new_X[y==3,0], new_X[y==3,1],marker='o')

# Here y =1,2,3 bcz y in target has 3 values, this visualisation will be applicale in case of classification data, maybe?

# graphs showing dataset before and after PCA

fig, axes = plt.subplots(1,2,figsize=(15,5))
axes[0].scatter(X_scaled[:,0], X_scaled[:,1], c=y)
axes[0].set_xlabel('x1')
axes[0].set_ylabel('x2')
axes[0].set_title('Before PCA')

axes[1].scatter(new_X[:,0], new_X[:,1], c=y)
axes[1].set_xlabel('PC1')
axes[1].set_ylabel('PC2')
axes[1].set_title('After PCA')
plt.show()

### USING SKLEARN

from sklearn.decomposition import PCA
pca=PCA(n_components=0.95)
X_pca=pca.fit_transform(X_scaled)

#  n_components=0.95---> This means that the goal is to capture 95% of the total variance in the data.

pca.components_.T[:,1] # transpose

pca.explained_variance_ratio_

# A visual way to view the cumulative variances is to plot a scree plot where a
# scree plot is a line plot of the principal components.

# plot a scree plot

components = len(pca.explained_variance_ratio_)
plt.plot(range(1,components+1), 
         np.cumsum(pca.explained_variance_ratio_ * 100))
plt.xlabel("Number of components");
plt.ylabel("Explained variance (%)");

X_pca.shape   # 41 Principal components will be used

pca.components_

## EXTRA ########:

from sklearn.decomposition import PCA
components = [1,2,3,4]

for n in components:
    pca = PCA(n_components = n, random_state=43)
    pca.fit(X_scaled)
    pca_df = pca.transform(X_scaled)
    print('Explained Variance Ratio For n_components={} is {}'.format(n,pca.explained_variance_ratio_.sum() * 100))
    
# components =[1,2,3,4] ----- we can change it

pca = PCA(n_components=4, random_state=43)
pca.fit(X_scaled)
pca_df = pd.DataFrame(pca.transform(X_scaled), columns=['PC1', 'PC2', 'PC3', 'PC4'])

pca_df

plt.bar(range(1,len(pca.explained_variance_)+1),pca.explained_variance_)
    
plt.plot([1]*(len(pca.explained_variance_)+1), 'r', linewidth=1)

plt.xlabel('PCA Feature')
plt.ylabel('Explained variance')
plt.title('Feature Explained Variance')
plt.show()

NOW WE WILL TAKE 2 COMPONENTS AS THEY ARE THE MINIMUM NUMBER OF PRINCIPAL COMPONENTS THAT EXPALIN THE MAXIMUM AMOUNT OF EXPLAINED VARIANCE

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X_scaled)
X_pca = pca.fit_transform(X_scaled,y)
plt.figure(figsize=(7,6));
plt.scatter(X_pca[:,0],X_pca[:,1],c=y);

# graphs showing dataset before and after PCA

fig, axes = plt.subplots(1,2,figsize=(15,5))
axes[0].scatter(X_scaled[:,0], X_scaled[:,1], c=y)
axes[0].set_xlabel('x1')
axes[0].set_ylabel('x2')
axes[0].set_title('Before PCA')

axes[1].scatter(X_pca[:,0], X_pca[:,1], c=y)
axes[1].set_xlabel('PC1')
axes[1].set_ylabel('PC2')
axes[1].set_title('After PCA')
plt.show()

## LDA:

Linear discriminant analysis is a method used to find a linear combination of features that separates classes in a dataset. It is commonly used for dimensionality reduction, as it can project a high-dimensional dataset onto a lower-dimensional space while still preserving the class separability. The resulting transformed data can be used for classification or visualization.

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda.fit(X_scaled, y)

exp_var_ratios = lda.explained_variance_ratio_

# Print the explained variance ratio for each component
for i, ratio in enumerate(exp_var_ratios):
    print(f"Component {i+1}: {ratio:.2f}")

plt.bar(range(len(exp_var_ratios)), exp_var_ratios)
plt.xlabel('LDA component')
plt.ylabel('Explained variance ratio')
plt.title('Explained Variance Ratio by LDA Component')
plt.show()

# Choosing n_components=2

lda=LinearDiscriminantAnalysis(n_components=2)

type(lda)

# Transforming the data

X_lda=lda.fit_transform(X_scaled,y)

X_lda

X_lda.shape

## Visualisation of the data

plt.scatter(X_lda[:,0],X_lda[:,1],c=y);

## EXTRA:############

# How many components to achieve % variation of the data?

lda1 = LinearDiscriminantAnalysis(n_components =4)
x_lda1 = lda1.fit_transform(X_scaled,y)
exp_var_lda1 = lda1.explained_variance_ratio_
cum_sum_eigenvalues1 = np.cumsum(exp_var_lda1)
print(cum_sum_eigenvalues1)

# 2 components will explain 77% variance

## LLE:

from sklearn.manifold import LocallyLinearEmbedding

lle=LocallyLinearEmbedding(n_neighbors=10,random_state=100)

The n_neighbors parameter in LocallyLinearEmbedding controls the number of neighboring points that are considered when constructing the locally linear relationship between data points. In general, the value of n_neighbors should be set high enough to capture the underlying manifold structure of the data but not too high to introduce too much noise or overshoot.

In practice, the choice of n_neighbors depends on the characteristics of the dataset and the desired tradeoff between preserving the global and local structures of the data. A typical rule of thumb is to set n_neighbors to be between 5 and 50, but this range can be expanded or contracted depending on the size and complexity of the dataset.

In the example provided, the n_neighbors value of 10 was chosen based on prior knowledge or experience with the dataset or possibly by experimenting with different values of n_neighbors to see which value provided the best results.

# Transforming the data,

X_lle=lle.fit_transform(X)
X_lle

# Visualisation

plt.scatter(X_lle[:,0],X_lle[:,1],c=y);

reconstruction_error = lle.reconstruction_error_
print("Reconstruction error:", reconstruction_error)

The reconstruction error of an LLE embedding measures the quality of the lower-dimensional representation of the data. It represents the average squared distance between the original data points and their reconstructed counterparts in the lower-dimensional space.

In the given code snippet, the reconstruction error of the LLE embedding is calculated using the reconstruction_error method and is found to be 9.53616278253001e-07.

Since the reconstruction error is very small (close to zero), it indicates that the LLE embedding is a good representation of the original data. This suggests that the LLE method was successful in preserving the local structure of the data in the lower-dimensional space. However, note that the interpretation of the reconstruction error may depend on the specific application and the scale of the data, and should be considered in conjunction with other evaluation metrics.

## TSNE:

from sklearn.manifold import TSNE
tsne=TSNE(n_components=2,perplexity=30)

TSNE is a dimensionality reduction technique that is often used for visualizing high-dimensional data in a lower-dimensional space (usually 2D or 3D) while preserving the structure of the data as much as possible.

The n_components parameter specifies the number of dimensions in the lower-dimensional space, which is set to 2 in this case to allow for easy visualization.

The perplexity parameter controls the balance between preserving local and global structure in the data. It is a hyperparameter that should be set based on the characteristics of the data and the desired outcome. In general, a higher perplexity value will preserve more global structure, while a lower perplexity value will preserve more local structure.

#  The perplexity is a parameter that balances the attention between local and global structure in the data. 
# In this case, the perplexity is set to 30.
# Perplexity is a measure of the effective number of neighbors in the high-dimensional space that are used in the t-SNE algorithm to 
# define the similarity between points. 

### EXTRA:#####

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# create an array of different n_components values to test
n_components = np.arange(2, 4)

# initialize an empty array to store the cost function values
cost_function = []

# loop over different n_components values
for n in n_components:
    # initialize t-SNE with the current n_components value
    tsne = TSNE(n_components=n, perplexity=30, random_state=100)
    # fit the data and calculate the cost function
    Y = tsne.fit_transform(X)
    cost_function.append(tsne.kl_divergence_)

# plot the cost function values as a function of n_components
plt.plot(n_components, cost_function)
plt.xlabel('Number of components')
plt.ylabel('Cost function')
plt.show()


# The point at which the curve starts to level off can be used as a heuristic for selecting the optimal value of n_components.

############################3

X_tsne=tsne.fit_transform(X_scaled)
X_tsne.shape

X_tsne[0]

# Visualisation of the transformed points

plt.scatter(X_tsne[:,0],X_tsne[:,1]);

plt.figure(figsize=(13,13))
plt.scatter(X_tsne[:,0],X_tsne[:,1],c=y);

# Creating a DF for better visualisation

X_df=pd.DataFrame({'X0':X_tsne[:,0],
                  'X1':X_tsne[:,1],
                  'Label':y})
X_df

plt.figure(figsize=(15,12))
sns.lmplot(data=X_df,x='X0',y='X1',hue='Label');

plt.figure(figsize=(15,12))
sns.lmplot(data=X_df,x='X0',y='X1',
           hue='Label',fit_reg=False);

## comparison? insights?

from sklearn.manifold import trustworthiness
from sklearn.manifold import TSNE

tsne = TSNE(perplexity=30)
x_tsne = tsne.fit_transform(X_scaled)

trustworthiness_score = trustworthiness(X_scaled, x_tsne, n_neighbors=2)
print("Trustworthiness score:", trustworthiness_score)

The trustworthiness score is a measure of how well the high-dimensional relationships between data points are preserved in the lower-dimensional representation. A score of 1 would indicate perfect preservation of relationships, while a score of 0 would indicate no preservation.

In this case, the t-SNE algorithm has produced a trustworthiness score of 0.9982697201017812, which is very close to 1. This indicates that the lower-dimensional representation is preserving the relationships between the data points very well, and suggests that the t-SNE embedding is a good representation of the original data

# comparison

PCA, LDA, t-SNE, and LLE are all dimensionality reduction techniques that can be used for different purposes. Therefore, the choice of technique and the comparison of results will depend on the specific problem being addressed.

PCA and LDA are linear methods and are suitable for data that have a linear structure. PCA is an unsupervised technique that can be used to reduce the dimensionality of data, while LDA is a supervised technique that can be used for feature extraction and classification.

t-SNE and LLE are non-linear methods that are useful when the data has a complex structure that cannot be captured by linear methods. t-SNE is an unsupervised technique that is commonly used for visualizing high-dimensional data, while LLE is also an unsupervised technique that can be used for non-linear dimensionality reduction.

To compare the results of these techniques, we can use different criteria such as visualization, clustering performance, classification performance, computational efficiency, and interpretability. For example, t-SNE is often used for visualization and can provide a more intuitive representation of the data, while PCA and LDA are often used for feature extraction and can provide a more interpretable representation of the data. Clustering and classification performance can also be used to compare the techniques, by evaluating the accuracy or other metrics on a labeled dataset.

Overall, the choice of technique and the comparison of results will depend on the specific problem being addressed and the evaluation criteria chosen.

# CLUSTERING

Clustering is a common technique in machine learning and data analysis that is used to group together similar data points based on their features or attributes.

There are several reasons why we may want to perform clustering:
Data exploration, Data compression, Anomaly detection, Customer segmentation,Recommendation systems

## HIERARCHICAL

Hierarchical clustering is a type of clustering algorithm that is based on a hierarchical decomposition of the data set into nested clusters. In other words, the algorithm groups the data into clusters in a hierarchical manner, starting from the individual data points and gradually merging them into larger clusters.

# COnverting to DF
X=pd.DataFrame(X, columns=X.columns)
X

# Drawing dendograms:
import scipy.cluster.hierarchy as sch

dendro=sch.dendrogram(sch.linkage(X,method='ward'))

plt.figure(figsize=(15,12))
dendro=sch.dendrogram(sch.linkage(X,method='ward'))
plt.axhline(y=220,color='red',linestyle='--');

plt.figure(figsize=(15,12))
dendro=sch.dendrogram(sch.linkage(X,method='ward'))
plt.axhline(y=220,color='red',linestyle='--');
plt.axhline(y=310,color='blue',linestyle='-.');

 the number of clusters is determined by examining the dendrogram and identifying the point where the distance between the clusters starts to increase rapidly.
HERE NUMBER = 2

The horizontal lines indicate possible choices for the number of clusters based on the dendrogram. The red line represents a relatively high threshold, suggesting a smaller number of clusters, while the blue line represents a lower threshold, suggesting a larger number of clusters. The specific values of 310 and 220 are arbitrary and may need to be adjusted depending on the data and problem at hand.

## Agglomerative Clustering using Sklearn (PART OF ABOVE CODE ONLY, WILL COME WITH HIERARCHICAL)

from sklearn.cluster import AgglomerativeClustering

clust=AgglomerativeClustering(n_clusters=2,linkage='ward')

'ward': Ward's minimum variance criterion. It minimizes the variance of the clusters being merged.
'complete': It uses the maximum distance between all observations of the two sets.
'average': Uses the average of the distances between each observation of the two sets.
'single': Uses the minimum distance between all observations of the two sets.

type(clust)

clust.fit_predict(X)

# The returns cluster label assignment for the corresponding sample in X.

clust.labels_

## Adding labels to the DF

X['Label']=pd.Series(clust.labels_)

X


# 2 clusters formed as labels

## Analysing the customer segments

# No of customers in each segment

X['Label'].value_counts()

# Listing all custmers belong to the segment '0'
X[X['Label']==0]

X[X['Label']==1]

# Buying pattern of Milk and Grocery

sns.scatterplot(x=X['Annual Income (k$)'],y=X['Age'],hue=X['Label']);

plt.figure(figsize=(15,12))
sns.pairplot(data=X,hue='Label',palette=(["#2f2f68", "#60abf3"]));

## make more graphs?


plt.figure(figsize=(16,5))
plt.title('Customers income by cluster')
ax = sns.boxplot(data=data2, x='Cluster', y='Income', palette='coolwarm', showfliers=False)
plt.show()

sns.countplot(data=data,x='clusters',palette=cluster_color)

## EXTRA: GRAPHS OF CLUSTERS


from sklearn.cluster import AgglomerativeClustering
clust = AgglomerativeClustering(n_clusters = 4, linkage = 'ward')

clust.fit_predict(X_scaled)

#Transposing the results over TSNE
plt.figure(figsize=(15,12));
plt.scatter(X_tsne[:,0],X_tsne[:,1],c=clust.labels_)

def agclust(n_clusters,linkage):
  c = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkage)
  c.fit_predict(X_scaled)
  plt.scatter(x_tsne[:,0],x_tsne[:,1],c=c.labels_)

agclust(4,'complete')

agclust(3,'ward')

## GMM

sns.pairplot(customer);

# start

### Finding no of clusters

n_components=np.arange(1,11)
aic_scores=[]
bic_scores=[]
for n in n_components:
    model=GaussianMixture(n,n_init=10)
    model.fit(X_scaled)
    aic_score=model.aic(X_scaled)
    bic_score=model.bic(X_scaled)
    aic_scores.append(aic_score)
    bic_scores.append(bic_score)

 evaluates the models using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).

plt.plot(n_components,aic_scores,label='AIC')
plt.plot(n_components,bic_scores,label='BIC')
plt.legend();

#### No of clusters=2

# Model building

gmm_data=GaussianMixture(n_components=2,n_init=10)
gmm_data.fit(X_scaled)

data_pred=gmm_data.predict(X_scaled)
data_pred

gmm_data.means_

gmm_data.covariances_

gmm_data.weights_

## Adding Cluster labels to DF

customer['Label']=data_pred

customer

customer['Label'].value_counts()

# Visuaise the clusters

sns.pairplot(customer, hue='Label');

## EXTRA:

def gmm(n_components,n_init,max_iter,init_params):
  gmm_data = GaussianMixture(n_components=n_components,n_init=n_init,max_iter=max_iter,init_params=init_params)
  gmm_data.fit(xs)
  data_pred=gmm_data.predict(xs)
  plt.scatter(x_tsne[:,0],x_tsne[:,1],c=data_pred)

gmm_data = GaussianMixture(n_components=3,n_init=10,max_iter=100,init_params='kmeans')
gmm_data.fit(X_scaled)
data_pred=gmm_data.predict(X_scaled)
plt.scatter(x_tsne[:,0],x_tsne[:,1],c=data_pred)
data['Class'] = data_pred
data.head()
data.drop(['Unnamed: 0'],axis=1, inplace=True)

## ANALYSE CLUSTERSSSSSSSSSSSSS

customer['clusters']=km.labels_

customer.head()

sns.countplot(data=data,x='clusters',palette=cluster_color)

plt.figure(figsize=(10,8))
sns.scatterplot(x='TotalMoneySpent',y=data.index,data=data,hue='clusters')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),frameon=False)
plt.ylabel('Frequency')
plt.title("Clustering on Total Expenditure")

plt.figure(figsize=(10,8))
sns.scatterplot(x='Income',y=data.index,data=data,hue='clusters')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),frameon=False)
plt.ylabel('Frequency')
plt.title("Clustering on Income")

plt.figure(figsize=(8,8))
sns.displot(data=data,x='Age',hue='clusters',kind='kde')
plt.title("Clustering based on Age")

plt.figure(figsize=(10,8))
sns.countplot(data=data,x='Education',hue='clusters',palette=cluster_color)
plt.legend(bbox_to_anchor=(1.01, 1),frameon=False)
plt.title("Clustering based on Education")

## DO CLUSTER PROFILING

# ????



# K Means Clustering

from sklearn.cluster import KMeans

ssd = []

for k in range(1,20):
  kmeans = KMeans(n_clusters=k, random_state=10)
  kmeans.fit(X_scaled)
  ssd.append(kmeans.inertia_)
plt.plot(range(1,20),ssd);

from sklearn.cluster import KMeans

kmeans_models = [KMeans(n_clusters=k, random_state=42).fit(X_pca) for k in range(2, 10)]


from sklearn.metrics import silhouette_score

plt.figure(figsize=(10,6))
silhoutte_scores = [silhouette_score(X_pca, model.labels_) for model in kmeans_models[1:4]]
plt.plot(range(2,5), silhoutte_scores, "bo-")
plt.xticks([2, 3, 4])
plt.title('Silhoutte scores vs Number of clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Silhoutte score')
plt.show()

from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=2, random_state=23)
kmeans.fit(X_pca)

print('Silhoutte score of our model is ' + str(silhouette_score(X_pca, kmeans.labels_)))

from yellowbrick.cluster import KElbowVisualizer

fig_elb = KElbowVisualizer(KMeans(random_state=42, n_init=10, max_iter=10000), k=(2,10))
fig_elb.fit(X_scaled)
print(f'Elbow value= {fig_elb.elbow_value_}')
fig_elb.show()

Sum_of_squared_distances = []
silhouette_scores = []
K = range(2,10) 
for _ in K:
    km = KMeans(n_clusters = _, n_init=10, random_state=42, max_iter=1000)
    y = km.fit_predict(pca_df)
    Sum_of_squared_distances.append(km.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled,y))

fig, ax1 = plt.subplots(figsize=(8,8))

color = 'tab:blue'
ax1.set_xlabel('K')
ax1.set_ylabel('Inertia', color=color)
ax1.plot(K, Sum_of_squared_distances, color=color)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  

color = 'tab:red'
ax2.set_ylabel('Silhouette Score', color=color)  
ax2.plot(K, silhouette_scores, color=color)
ax2.tick_params(axis='y', labelcolor=color)

fig.tight_layout()
plt.show()

## analyse ##333 and see if clustering is proper

card['cluster_id'] = kmeans.labels_

plt.figure(figsize=(11,7))
sns.scatterplot(data=card, x='ONEOFF_PURCHASES', y='PURCHASES', hue='cluster_id')
plt.title('Distribution of clusters based on One off purchases and total purchases')
plt.show()

plt.figure(figsize=(11,7))
sns.scatterplot(data=card, x='CREDIT_LIMIT', y='PURCHASES', hue='cluster_id')
plt.title('Distribution of clusters based on Credit limit and total purchases')
plt.show()







# DBSCAN

from sklearn.neighbors import NearestNeighbors # importing the library
neighb = NearestNeighbors(n_neighbors=2) # creating an object of the NearestNeighbors class
nbrs=neighb.fit(X_scaled) # fitting the data to the object
distances,indices=nbrs.kneighbors(X_scaled)

distances = np.sort(distances, axis = 0) # sorting the distances
distances = distances[:, 1] # taking the second column of the sorted distances
plt.rcParams['figure.figsize'] = (12,7) # setting the figure size
plt.plot(distances) # plotting the distances
plt.show() # showing the plot

from sklearn.cluster import DBSCAN
# cluster the data into five clusters
dbscan = DBSCAN(eps = 5000, min_samples = 4).fit(X_scaled) # fitting the model
labels = dbscan.labels_ # getting the labels

# Plot the clusters
plt.figure(figsize=(12,7))
plt.scatter(x=X_scaled[:,3],y=y, c = labels, cmap= "plasma") # plotting the clusters
plt.xlabel("Cash Advance") # X-axis label
plt.ylabel("Purchases") # Y-axis label
plt.show() # showing the plot

## not clustering properly

from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=25)
nbrs = neigh.fit(X_scaled)
distances, indices = nbrs.kneighbors(X_scaled)

# Plotting K-distance Graph
distances = np.sort(distances, axis=0)
# distances = distances[:,1]
plt.figure(figsize=(10,10))
plt.plot(distances)
plt.title('K-distance Graph',fontsize=20)
plt.xlabel('Data Points sorted by distance',fontsize=14)
plt.ylabel('Epsilon',fontsize=14)
plt.show()

