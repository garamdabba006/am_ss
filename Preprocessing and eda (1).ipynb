{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df06199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0377f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db29c806",
   "metadata": {},
   "source": [
    "1.\tPerform following pre-processing on the dataset: \n",
    "\n",
    "•\tDrop the columns ‘ID’ and ‘Var_1’. \n",
    "\n",
    "•\tCheck if there are any null values and remove all records having null values. \n",
    "\n",
    "•\tDrop all rows having Family_Size values as 7.0, 8.0 and 9.0\n",
    "\n",
    "•Formulate a Deep Learning problem by taking ‘Segmentation’ as the target and other columns as  features. Is it classification or regression? Justify\n",
    "\n",
    "•\tConvert all categorical features into numeric and normalise them\n",
    "\n",
    "•\tTransform the target values to respective labels\n",
    "\n",
    "•\tSplit the feature set and target into train and test with 80:20 ratio. Use random_state=10 (throughout the experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns ‘ID’ and ‘Var_1’\n",
    "df=df.drop(['ID','Var_1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed60630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all records having null values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d568cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3356140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows having Family_Size values as 7.0, 8.0 and 9.0\n",
    "index = df[ (df['Family_Size'] == 7.0) & (df['Family_Size'] == 8.0) & (df['Family_Size'] == 9.0) ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index , inplace=True)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical features into numeric and normalise them\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "df['Ever_Married'] = le.fit_transform(df['Ever_Married'])\n",
    "df['Graduated'] = le.fit_transform(df['Graduated'])\n",
    "df['Profession'] = le.fit_transform(df['Profession'])\n",
    "df['Spending_Score'] = le.fit_transform(df['Spending_Score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the target values to respective labels:\n",
    "df['Segmentation'] = le.fit_transform(df['Segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target\n",
    "y=df['Segmentation']\n",
    "# Features\n",
    "X=df.drop(['Segmentation'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad439c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af895bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of features  required\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e48c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRe-processing\n",
    "# Drop null values\n",
    "bank.dropna(inplace=True)\n",
    "\n",
    "# Remove two columns name is 'default' and 'loan'\n",
    "bank=bank.drop(['default', 'loan'], axis=1)\n",
    "\n",
    "# get names of indexes for which\n",
    "# column duration has value < 60\n",
    "index_names = bank[ bank['duration'] < 60 ].index\n",
    "# drop these row indexes\n",
    "# from dataFrame\n",
    "bank.drop(index_names, inplace = True)\n",
    "\n",
    "\n",
    "# count the number of occurrences of each job type\n",
    "job_counts = bank[\"job\"].value_counts()\n",
    "# filter out the job types that have a count less than 2\n",
    "jobs_to_drop = job_counts[job_counts < 2].index.tolist()\n",
    "# drop the rows that correspond to the filtered job types from the original DataFrame\n",
    "bank.drop(bank[bank[\"job\"].isin(jobs_to_drop)].index, inplace=True)\n",
    "\n",
    "# print the modified DataFrame\n",
    "print(bank)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "# Encode the 'job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome', and 'y' columns\n",
    "bank['job'] = le.fit_transform(bank['job'])\n",
    "bank['marital'] = le.fit_transform(bank['marital'])\n",
    "bank['education'] = le.fit_transform(bank['education'])\n",
    "bank['housing'] = le.fit_transform(bank['housing'])\n",
    "bank['contact'] = le.fit_transform(bank['contact'])\n",
    "bank['month'] = le.fit_transform(bank['month'])\n",
    "bank['poutcome'] = le.fit_transform(bank['poutcome'])\n",
    "bank['y'] = le.fit_transform(bank['y'])\n",
    "bank\n",
    "\n",
    "#checking for null values\n",
    "bank.isnull().sum()\n",
    "\n",
    "#filling null values\n",
    "#house['City'].fillna(method='ffill',inplace=True)\n",
    "#house['Facing'].fillna(value=\"North\",inplace=True)\n",
    "#df[column].fillna(value=df[column].mean(), inplace=True)\n",
    "\n",
    "#Target\n",
    "y=bank['y']\n",
    "# Features\n",
    "X=bank.drop(['y'],axis=1)\n",
    "\n",
    "\n",
    "#Normalization of features if required\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X_1)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775efab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b86d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA:\n",
    "df['booking_status'].value_counts()\n",
    "df.describe()\n",
    "# DATA SUMMARY:\n",
    "\n",
    "def summary(df):\n",
    "    print(f'data shape: {df.shape}')\n",
    "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
    "    summ['#missing'] = df.isnull().sum().values * 100\n",
    "    summ['%missing'] = df.isnull().sum().values / len(df)\n",
    "    summ['#unique'] = df.nunique().values\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "    summ['min'] = desc['min'].values\n",
    "    summ['max'] = desc['max'].values\n",
    "    summ['first value'] = df.loc[0].values\n",
    "    summ['second value'] = df.loc[1].values\n",
    "    summ['third value'] = df.loc[2].values\n",
    "    \n",
    "    return summ\n",
    "    pd.set_option('display.max_rows', None)\n",
    "summary(df)\n",
    "\n",
    "# handling unique values:\n",
    "unique_stats = df.nunique().reset_index().rename(columns ={'index': 'feature', 0: 'nunique'})\n",
    "unique_stats[unique_stats['nunique'] == 1]\n",
    "\n",
    "# eda:\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(df.corr(), annot=False);\n",
    "\n",
    "#freq:\n",
    "plt.hist(x=data['Recency'],edgecolor='white',color='#245D')\n",
    "plt.title('Distribution of their last visit')\n",
    "plt.xlabel('Last Visit (In days)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "sizes = data['Complain'].value_counts() \n",
    "labels = 'No compain', 'Complain' \n",
    "ax.pie(sizes,labels=labels,autopct='%1.1f%%',shadow=True, startangle=45,wedgeprops={'edgecolor':'black'},\n",
    "        colors=['#3185FC','#F5F5F5'])\n",
    "ax.axis('equal')\n",
    "ax.set_title('Complains raised by the customer')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.title(f'Customers income boxplot')\n",
    "ax = sns.boxplot(data['Income'], color='green')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# shuffling dta:\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "\n",
    "# deleting duplicate:\n",
    "data.duplicated().sum()\n",
    "data[data.duplicated(keep=False)].sort_values(by='Income')\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.duplicated().sum()\n",
    "\n",
    "data['Marital_Status'].value_counts()\n",
    "\n",
    "data_dummies=pd.get_dummies(data1[['Age_Group','Partner','Education_Level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5476bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "bank = bank.dropna()\n",
    "\n",
    "# Remove two columns name is 'default' and 'loan'\n",
    "bank=bank.drop(['default', 'loan'], axis=1)\n",
    "\n",
    "# get names of indexes for which\n",
    "# column duration has value < 60\n",
    "index_names = bank[ bank['duration'] < 60 ].index\n",
    "# drop these row indexes\n",
    "# from dataFrame\n",
    "bank.drop(index_names, inplace = True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the 'job', 'marital', and 'y' columns\n",
    "bank['job'] = le.fit_transform(bank['job'])\n",
    "bank['marital'] = le.fit_transform(bank['marital'])\n",
    "bank['y'] = le.fit_transform(bank['y'])\n",
    "bank\n",
    "\n",
    "#checking for null values\n",
    "bank.isnull().sum()\n",
    "\n",
    "#filling null values\n",
    "#house['City'].fillna(method='ffill',inplace=True)\n",
    "#house['Facing'].fillna(value=\"North\",inplace=True)\n",
    "#df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "#Target\n",
    "y=bank['y']\n",
    "# Features\n",
    "X=bank.drop(['y'],axis=1)\n",
    "\n",
    "#Normalization of features if required\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X_1)\n",
    "X_scaled\n",
    "\n",
    "# SHUFFLING\n",
    "from sklearn.utils import shuffle\n",
    "df=shuffle(df)\n",
    "df\n",
    "\n",
    "1.\tData.info()\n",
    "2.\tData.describe()\n",
    "3.\tData.isnull.sum() – total number of null values in each column\n",
    "4.\tdata['company'].isnull().sum() – count of null values in ‘company’ column\n",
    "5.\tdata.drop(['company'],axis=1, inplace=True) – drop company column\n",
    "6.\tdf['Sale'].fillna(df['Sale'].mean(), inplace=True) – fill empty rows in sale column with mean value\n",
    "\n",
    "# RESAMPLING DATA\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "data_1 = data[data['HeartDiseaseorAttack']==1]\n",
    "data_0 = data[data['HeartDiseaseorAttack']==0]\n",
    "\n",
    "data_0.shape, data_1.shape\n",
    "\n",
    "data_0 has only [data['HeartDiseaseorAttack']==0\n",
    "data_1 has only [data['HeartDiseaseorAttack']==1\n",
    "\n",
    "down_data_0 = resample(data_0, replace = False, random_state = 100, n_samples = 5000)\n",
    "down_data_1 = resample(data_1, replace = False, random_state = 100, n_samples = 5000)\n",
    "down_data_0.shape, down_data_1.shape\n",
    "\n",
    "data_new = pd.concat([down_data_0, down_data_1])\n",
    "\n",
    "#converting data_0 and data_1 to a new dataframe with 5000 samples\n",
    "(above code is for downsampling, incase of upsampling --- replace = True)\n",
    "\n",
    "# INTO DATAFRAME\n",
    "\n",
    "# initialize data of lists.\n",
    "data = {'Model': ['model_1', 'model_2', 'model_3', 'model_4', 'model_5' , 'model_6'], 'Test Loss': [1.0797, 1.0777, 1.067, 1.0705420970916748, 1.063335657119751 ,  1.0641319751739502 ]}\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c3110",
   "metadata": {},
   "source": [
    "## VISUALISATION/ EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(df.corr(), annot=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44673ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2ecbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6829a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
